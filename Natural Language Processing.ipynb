{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8b7b46",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04486de8",
   "metadata": {},
   "source": [
    "<img src=\"https://codesrevolvewordpress.s3.us-west-2.amazonaws.com/revolveai/2022/05/15110810/natural-language-processing-techniques.png\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744ea97",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50f48f",
   "metadata": {},
   "source": [
    "> #### Field of study focused on making sense of language - Using statistics and computers\n",
    "> is a subfield of Linguistics, Computer Science (CS), and Artificial Intelligence (AI) concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "<br><br>\n",
    "> Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.\n",
    "<br><br>\n",
    "> **Methods**: Rules, statistics, neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c7116",
   "metadata": {},
   "source": [
    "# Regular Expressions: Regexes in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3572670a",
   "metadata": {},
   "source": [
    "### A (Very Brief) History of Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5a483",
   "metadata": {},
   "source": [
    "In 1951, mathematician Stephen Cole Kleene described the concept of a regular language, a language that is recognizable by a finite automaton and formally expressible using regular expressions. In the mid-1960s, computer science pioneer Ken Thompson, one of the original designers of Unix, implemented pattern matching in the QED text editor using Kleene’s notation.\n",
    "\n",
    "Since then, regexes have appeared in many programming languages, editors, and other tools as a means of determining whether a string matches a specified pattern. Python, Java, and Perl all support regex functionality, as do most Unix tools and many text editors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe2895",
   "metadata": {},
   "source": [
    "### The re Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce39de",
   "metadata": {},
   "source": [
    "Regex functionality in Python resides in a module named re. The re module contains many useful functions and methods, most of which you’ll learn about in the next tutorial in this series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee045397",
   "metadata": {},
   "source": [
    "For now, you’ll focus predominantly on one function, re.search().\n",
    "\n",
    "re.search(, )\n",
    "\n",
    "re.search(, ) scans looking for the first location where the pattern matches. If a match is found, then re.search() returns a match object. Otherwise, it returns None.\n",
    "\n",
    "re.search() takes an optional third argument that you’ll learn about at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c803d9",
   "metadata": {},
   "source": [
    "# Introduction to tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f701a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe97ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8ab6d1a",
   "metadata": {},
   "source": [
    "# nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac5037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511fba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc97d2e",
   "metadata": {},
   "source": [
    "# Word counts with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6658622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe16b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e62ce03",
   "metadata": {},
   "source": [
    "### Simple text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13ed990e",
   "metadata": {},
   "source": [
    "### Introduction to gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089e708",
   "metadata": {},
   "source": [
    "> ### What is gensim?\n",
    "> Popular open-source NLP library\n",
    "<br><br>\n",
    "> Uses top academic models to perform complex tasks\n",
    "<br><br>\n",
    ">Building document or word vectors\n",
    "<br><br>\n",
    "Performing topic identi,cation and document comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d661f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7cdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = [\n",
    "    'The movie was about a spaceship and aliens.',\n",
    "    'I really liked the movie!',\n",
    "    'Awesome action scenes, but boring characters.',\n",
    "    'The movie was awful! I hate alien films.',\n",
    "    'Space is cool! I liked the movie.',\n",
    "    'More space films, please!',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0e106a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
       " ['i', 'really', 'liked', 'the', 'movie', '!'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '!']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower())for doc in my_documents]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb33990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506a7c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ffac291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a gensim corpus\n",
    "corpus =[dictionary.doc2bow(doc) for doc in tokenized_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16124424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6b939",
   "metadata": {},
   "source": [
    "- gensim models can be easily saved, updated, and reused\n",
    "- Our dictionary can also be updated\n",
    "- This more advanced and feature rich bag-of-words can beused in future exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8bc7d",
   "metadata": {},
   "source": [
    "### Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1634e7d",
   "metadata": {},
   "source": [
    "> ### What is tf-idf?\n",
    "> Term frequency - inverse document frequency\n",
    "<br><br>\n",
    "> Allows you to determine the most important words in each document\n",
    "<br><br>\n",
    "> Each corpus may have shared words beyond just stopwords\n",
    "<br><br>\n",
    "> These words should be down-weighted in importance\n",
    "<br><br>\n",
    "> Example from astronomy: \"Sky\n",
    "<br><br>\n",
    "> Ensures most common words don't show up as key words\n",
    "<br><br>\n",
    "> Keeps document speci,c frequent words weighted high\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d8586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d123a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d9d9d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.1746298276735174),\n",
       " (7, 0.1746298276735174),\n",
       " (9, 0.1746298276735174),\n",
       " (10, 0.29853166221463673),\n",
       " (11, 0.47316148988815415),\n",
       " (12, 0.7716931521027908)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f38b6",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820cd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5b39fe",
   "metadata": {},
   "source": [
    "### Using nltk for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef0c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae5054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c53c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57840d3b",
   "metadata": {},
   "source": [
    "### Introduction to SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4a677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953caa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ce6ff39",
   "metadata": {},
   "source": [
    "### Multilingual NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb99bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f07396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
